# AI Ассистент с MCP Серверами

Веб-приложение на FastAPI с чат-ботом LLM и упрощенной архитектурой MCP серверов для работы с Jira, Atlassian Confluence, GitLab и LDAP. Включает интеграцию с Active Directory для корпоративной аутентификации.

## Новая упрощенная архитектура

Система использует классическую схему работы с инструментами:
- Каждый MCP сервер имеет фиксированный набор инструментов
- При запросе пользователя все доступные инструменты отправляются в LLM сразу
- LLM самостоятельно выбирает подходящий инструмент и вызывает его
- Убрана сложная система интеллектуального определения намерений

## Возможности

- 🤖 **AI Чат-бот** с поддержкой русских команд
- 🔐 **Аутентификация через Active Directory** - корпоративная безопасность
- ⚙️ **Админ-панель** - управление настройками всех сервисов
- 🔧 **MCP Сервер для Jira** - создание, поиск и управление задачами
- 📄 **MCP Сервер для Confluence** - создание и поиск страниц документации
- 🐙 **MCP Сервер для GitLab** - управление проектами, коммитами и ветками
- 📊 **Анализ кода по задачам** - комплексный анализ разработки с интеграцией всех сервисов
- 🌐 **Современный веб-интерфейс** с Bootstrap и Font Awesome
- 🐳 **Docker контейнеры** для легкого развертывания
- 📊 **Автоматическая документация API** (Swagger/OpenAPI)
- 🔒 **JWT токены и сессии** для безопасной работы

## Архитектура

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Веб-клиент    │────│   FastAPI App   │────│   LLM Server    │
│   (HTML/JS)     │    │   (Python)      │    │   (Ollama)      │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                              │
                    ┌─────────┼─────────┐
                    │         │         │
            ┌───────▼───┐ ┌───▼───┐ ┌───▼───┐ ┌───▼───┐
            │Jira MCP   │ │Confluence│ │GitLab │ │ LDAP │
            │Server     │ │MCP     │ │MCP    │ │MCP   │
            └───────────┘ └────────┘ └───────┘ └───────┘
```

### Упрощенный поток обработки

1. **Пользователь** отправляет сообщение в чат
2. **FastAPI** получает запрос и передает в упрощенный MCP клиент
3. **MCP клиент** собирает все доступные инструменты от всех серверов
4. **LLM** получает сообщение + полный список инструментов
5. **LLM** выбирает подходящий инструмент и формирует JSON запрос
6. **MCP клиент** выполняет выбранный инструмент
7. **Результат** форматируется и возвращается пользователю

## Установка и запуск

### 1. Клонирование и настройка

```bash
git clone <repository-url>
cd MCP
cp env.example .env
```

### 2. Настройка переменных окружения

Отредактируйте файл `.env`:

```env
# LLM Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Jira Configuration
JIRA_URL=https://your-domain.atlassian.net
JIRA_USERNAME=your-email@example.com
JIRA_API_TOKEN=your-api-token

# Atlassian Configuration
ATLASSIAN_URL=https://your-domain.atlassian.net
ATLASSIAN_USERNAME=your-email@example.com
ATLASSIAN_API_TOKEN=your-api-token

# GitLab Configuration
GITLAB_URL=https://gitlab.com
GITLAB_TOKEN=your-gitlab-token

# Active Directory Configuration
AD_SERVER=ldap://your-domain.local:389
AD_DOMAIN=your-domain.local
AD_BASE_DN=CN=Users,DC=your-domain,DC=local

# JWT Configuration
JWT_SECRET=your-super-secret-jwt-key-here
JWT_ALGORITHM=HS256
JWT_EXPIRE_HOURS=24

# Session Configuration
REDIS_URL=redis://localhost:6379
SESSION_EXPIRE_HOURS=24
```

### 3. Запуск с Docker Compose

```bash
# Запуск всех сервисов
docker-compose up -d

# Просмотр логов
docker-compose logs -f

# Остановка
docker-compose down
```

### 4. Ручная установка (альтернатива)

```bash
# Установка зависимостей
pip install -r requirements.txt

# Запуск LLM сервера (в отдельном терминале)
docker run -d -p 11434:11434 ollama/ollama:latest

# Загрузка модели
ollama pull llama2

# Запуск веб-приложения
python app.py
```

## Использование

1. Откройте браузер и перейдите на `http://localhost:5000`
2. Используйте быстрые команды или вводите команды вручную
3. Следите за статусом сервисов в левой панели

### Примеры команд

**Jira:**
- "создай задачу в jira"
- "найди задачи по ключевому слову"
- "покажи все задачи"
- "измени статус задачи TEST-123"

**Confluence:**
- "создай страницу в confluence"
- "найди страницы по ключевому слову"
- "покажи все страницы"
- "обнови страницу с ID 123456"

**GitLab:**
- "создай проект в gitlab"
- "найди проекты по ключевому слову"
- "покажи коммиты проекта название"
- "покажи ветки проекта название"

## API Endpoints

### Основные
- `GET /` - Главная страница (требует аутентификации)
- `GET /login` - Страница входа
- `GET /admin` - Админ-панель
- `GET /docs` - Документация API (Swagger UI)

### Аутентификация
- `POST /api/auth/login` - Вход в систему через Active Directory
- `POST /api/auth/logout` - Выход из системы
- `GET /api/auth/me` - Информация о текущем пользователе

### Админ-панель
- `POST /api/admin/login` - Вход в админ-панель
- `GET /api/admin/info` - Информация об админе
- `GET /api/admin/config` - Получение конфигурации
- `POST /api/admin/config/update` - Обновление конфигурации
- `POST /api/admin/test-connection` - Тестирование подключений
- `POST /api/admin/change-password` - Смена пароля админа

### Чат и MCP серверы
- `POST /api/chat` - Отправка сообщения чат-боту (требует аутентификации)
- `GET /api/health` - Проверка статуса сервисов

### Анализ кода
- `POST /api/analyze-code` - Анализ кода по задаче Jira (требует аутентификации)

## Структура проекта

```
MCP/
├── app.py                    # Основное FastAPI приложение
├── models.py                # Pydantic модели для API
├── llm_client.py            # Клиент для работы с LLM (обновлен)
├── mcp_client.py            # Упрощенный MCP клиент (новый)
├── test_simple_architecture.py # Тест новой архитектуры
├── auth/                    # Модули аутентификации
│   ├── __init__.py
│   ├── ad_auth.py           # Интеграция с Active Directory
│   ├── admin_auth.py        # Аутентификация админа
│   ├── session_manager.py   # Управление сессиями
│   └── middleware.py        # Middleware для авторизации
├── config/                  # Управление конфигурацией
│   ├── __init__.py
│   └── config_manager.py    # Менеджер конфигурации
├── analyzers/               # Анализаторы кода
│   ├── __init__.py
│   └── code_analyzer.py     # Анализатор кода по задачам
├── mcp_servers/             # MCP серверы (обновлены)
│   ├── __init__.py
│   ├── jira_server.py       # MCP сервер для Jira
│   ├── atlassian_server.py  # MCP сервер для Confluence
│   ├── gitlab_server.py     # MCP сервер для GitLab
│   └── ldap_server.py       # MCP сервер для LDAP
├── templates/
│   ├── index.html           # Главная страница
│   ├── login.html           # Страница входа
│   └── admin.html           # Админ-панель
├── static/                  # Статические файлы
├── requirements.txt         # Python зависимости
├── docker-compose.yml       # Docker Compose конфигурация
├── Dockerfile              # Docker образ для приложения
├── run.py                  # Скрипт запуска
├── init_model.py           # Инициализация LLM модели
└── README.md              # Документация
```

### Ключевые изменения в архитектуре

- **Удален** `intent_analyzer.py` - больше не нужен интеллектуальный анализ
- **Удален** `mcp_client.py` - заменен на упрощенную версию
- **Добавлен** `mcp_client.py` - новый упрощенный MCP клиент
- **Обновлены** все MCP серверы - добавлены методы `call_tool()`
- **Обновлен** `llm_client.py` - добавлена поддержка инструментов

## Требования

- Python 3.11+
- Docker и Docker Compose
- Доступ к Jira, Confluence и GitLab (для MCP серверов)
- Active Directory сервер (для аутентификации)
- Redis сервер (для сессий)
- LLM провайдер (OpenAI, Anthropic, Google, Ollama или локальный)

### LLM Провайдеры

Система поддерживает различные LLM провайдеры:

- **OpenAI**: GPT-4, GPT-3.5, GPT-4o-mini
- **Anthropic**: Claude-3.5-Sonnet, Claude-3-Haiku
- **Google**: Gemini-1.5-Flash, Gemini-1.5-Pro
- **Ollama**: Локальные модели (Llama, Mistral, CodeLlama)
- **Local**: Пользовательские API

Подробнее см. [LLM_PROVIDERS.md](LLM_PROVIDERS.md)

### Управление сервисами

Система поддерживает гибкое управление сервисами:

- **LDAP/Active Directory**: Можно отключить через настройку `enabled: false`
- **Jira, GitLab, Confluence**: Настраиваются индивидуально
- **LLM провайдеры**: Переключаются через API

#### Отключение LDAP

Для отключения LDAP установите в `app_config.json`:

```json
{
  "active_directory": {
    "enabled": false
  }
}
```

Или используйте API:

```bash
curl -X POST http://localhost:8000/api/ldap/toggle \
  -H "Content-Type: application/json" \
  -d '{"enabled": false}'
```

## Безопасность

- 🔐 **Аутентификация через Active Directory** - интеграция с корпоративной системой
- 🔒 **JWT токены** - безопасная передача данных о пользователе
- 🍪 **Сессии с Redis** - управление состоянием пользователей
- 🛡️ **Middleware авторизации** - защита всех API endpoints
- 👥 **Проверка групп AD** - контроль доступа на основе ролей
- ⏰ **Автоматическое истечение сессий** - безопасность при неактивности

## Админ-панель

Админ-панель доступна по адресу `/admin` и позволяет управлять всеми настройками приложения:

### Доступ к админ-панели
- **URL:** `http://localhost:5000/admin`
- **Логин по умолчанию:** `admin`
- **Пароль по умолчанию:** `admin`

### Возможности админ-панели

1. **🔧 Управление настройками сервисов:**
   - Active Directory (сервер, домен, Base DN)
   - Jira (URL, учетные данные, API токен)
   - Atlassian Confluence (URL, учетные данные, API токен)
   - GitLab (URL, токен)
   - LLM сервер (URL, модель)
   - Redis (URL подключения)

2. **🧪 Тестирование подключений:**
   - Проверка доступности каждого сервиса
   - Валидация учетных данных
   - Отображение статуса подключений

3. **🔐 Управление админом:**
   - Смена пароля админа
   - Просмотр информации об аккаунте
   - История изменений

4. **💾 Сохранение конфигурации:**
   - Автоматическое сохранение в JSON файл
   - Валидация настроек
   - Откат к предыдущим версиям

## Анализ кода по задачам

Система анализа кода позволяет получить комплексную аналитику разработки по любой задаче Jira:

### Возможности анализа

1. **📊 Статистика разработки:**
   - Количество коммитов
   - Строки кода (добавлено/удалено)
   - Количество измененных файлов
   - Длительность разработки

2. **👥 Анализ авторов:**
   - Основные разработчики
   - Статистика по каждому автору
   - Вклад в проект

3. **⏰ Временные рамки:**
   - Дата первого коммита
   - Дата последнего коммита
   - Общая длительность разработки

4. **🔗 Интеграция сервисов:**
   - Информация о задаче из Jira
   - Коммиты из GitLab
   - Связанные страницы Confluence

### Использование

**Через чат-бот:**
```
проанализируй код под задачу №PROJ-123
анализ задачи PROJ-456
анализируй код задачи №TEST-789
```

**Через веб-интерфейс:**
1. Нажмите кнопку "Анализ кода" в боковой панели
2. Введите номер задачи в формате PROJ-123
3. Нажмите "Анализировать"
4. Получите детальный отчет

**Через API:**
```bash
curl -X POST "http://localhost:5000/api/analyze-code" \
  -H "Content-Type: application/json" \
  -d '{"task_key": "PROJ-123"}'
```

### Пример отчета

```
📊 ОТЧЕТ ПО АНАЛИЗУ КОДА ЗАДАЧИ PROJ-123

📋 Информация о задаче:
• Название: Реализация авторизации пользователей
• Статус: В работе
• Исполнитель: Иван Петров
• Создана: 15.01.2024 10:30
• Обновлена: 20.01.2024 14:45

💻 Статистика разработки:
• Всего коммитов: 12
• Строк добавлено: 1,250
• Строк удалено: 89
• Файлов изменено: 8
• Длительность разработки: 5 дней

👥 Авторы:
• 1. Иван Петров: 8 коммитов, +950 -45 строк
• 2. Мария Сидорова: 4 коммита, +300 -44 строки

⏰ Временные рамки:
• Первый коммит: 15.01.2024 11:20
• Последний коммит: 20.01.2024 14:30

📄 Связанные страницы Confluence (2):
• Техническое задание на авторизацию
• API документация для аутентификации
```

## Устранение неполадок

### LLM сервер недоступен
```bash
# Проверьте статус контейнера
docker ps | grep ollama

# Перезапустите контейнер
docker-compose restart llm-server

# Проверьте логи
docker-compose logs llm-server
```

### MCP серверы не работают
1. Проверьте переменные окружения в `.env`
2. Убедитесь в правильности API токенов
3. Проверьте доступность внешних сервисов

### Проблемы с портами
- Веб-приложение: `http://localhost:5000`
- LLM сервер: `http://localhost:11434`

## Разработка

Для разработки рекомендуется:

1. Использовать виртуальное окружение Python
2. Запускать LLM сервер в Docker
3. Запускать веб-приложение локально с `python app.py`

## Лицензия

MIT License
